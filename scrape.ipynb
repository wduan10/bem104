{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b9843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from functools import lru_cache\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os, json, time, tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42380306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIR',\n",
       " '4165A',\n",
       " 'ADCT.1',\n",
       " 'IWKS',\n",
       " 'ALO.2',\n",
       " 'AEN.2',\n",
       " 'AAL',\n",
       " '4267A',\n",
       " 'CECE',\n",
       " 'ARXX']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_statements = pd.read_csv(\"data/annual_statements.csv\")\n",
    "all_tickers = df_statements['tic'].unique().tolist()\n",
    "all_tickers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c54fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 'data/monthly_returns.csv' with 460 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['4165A']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['IWKS']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ALO.2']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ALO-2']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ALO']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['AEN.2']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['AEN-2']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['AEN']: YFPricesMissingError('possibly delisted; no price data found  (1mo 2003-01-01 -> 2025-08-01)')\n",
      "\n",
      "1 Failed download:\n",
      "['4267A']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['CECE']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ARXX']: YFPricesMissingError('possibly delisted; no price data found  (1mo 2003-01-01 -> 2025-08-01)')\n",
      "\n",
      "1 Failed download:\n",
      "['AVX']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ACETQ']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['AHIXD']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI.1']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI-1']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    }
   ],
   "source": [
    "START = \"2003-01-01\"\n",
    "# yfinance's `end` is exclusive; use Aug 1 to include all of July 2025\n",
    "END   = \"2025-08-01\"\n",
    "OUT_PATH = \"data/monthly_returns.csv\"\n",
    "LOG_PATH = \"data/monthly_returns.log.jsonl\"\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "\n",
    "def normalize_for_yahoo(sym: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Produce candidate Yahoo symbols from a Compustat-like ticker.\n",
    "    e.g., 'BRK.B' -> ['BRK-B','BRK.B'], 'GKIN.1' -> ['GKIN','GKIN-1','GKIN.1']\n",
    "    \"\"\"\n",
    "    s = str(sym).strip().upper()\n",
    "    cands = [s]\n",
    "    if \".\" in s:\n",
    "        cands.append(s.replace(\".\", \"-\"))\n",
    "        # drop trailing .<digits> like GKIN.1\n",
    "        if re.search(r\"\\.\\d+$\", s):\n",
    "            cands.append(re.sub(r\"\\.\\d+$\", \"\", s))\n",
    "    if \"/\" in s:\n",
    "        cands.append(s.replace(\"/\", \"-\"))\n",
    "    # de-dup while preserving order\n",
    "    seen, out = set(), []\n",
    "    for x in cands:\n",
    "        if x and x not in seen:\n",
    "            out.append(x); seen.add(x)\n",
    "    return out\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _download_monthly_adjclose(sym: str, start: str = START, end: str = END) -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Try normalized candidates; return a clean monthly adjusted close Series or None.\n",
    "    \"\"\"\n",
    "    for cand in normalize_for_yahoo(sym):\n",
    "        try:\n",
    "            df = yf.download(cand, start=start, end=end, interval=\"1mo\",\n",
    "                             auto_adjust=True, progress=False, keepna=False)\n",
    "            if not df.empty:\n",
    "                col = \"Adj Close\" if \"Adj Close\" in df.columns else \"Close\"\n",
    "                px = df[col].dropna()\n",
    "                if not px.empty:\n",
    "                    px.index = px.index.to_period('M')\n",
    "                    return px\n",
    "        except Exception:\n",
    "            # swallow and try next candidate\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _atomic_save_csv(df: pd.DataFrame, path: str):\n",
    "    \"\"\"Write CSV atomically to avoid partial files on interruption.\"\"\"\n",
    "    dir_ = os.path.dirname(path) or \".\"\n",
    "    with tempfile.NamedTemporaryFile(\"w\", dir=dir_, delete=False, suffix=\".tmp\") as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        df.to_csv(tmp_path, index=True)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "def _append_log(event: dict, path: str = LOG_PATH):\n",
    "    event = dict(event)\n",
    "    event[\"ts\"] = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    with open(path, \"a\") as f:\n",
    "        f.write(json.dumps(event) + \"\\n\")\n",
    "\n",
    "def monthly_returns_df(tickers: list[str],\n",
    "                       start: str = START,\n",
    "                       end: str = END,\n",
    "                       min_non_na: int = 1,\n",
    "                       as_period_index: bool = True,\n",
    "                       out_path: str = OUT_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build monthly returns for tickers from Jan 2003 to Jul 2025.\n",
    "    Checkpoints to CSV after each ticker, and resumes from that CSV if present.\n",
    "    Columns are the ORIGINAL tickers you passed in.\n",
    "    \"\"\"\n",
    "    # Master monthly grid\n",
    "    full_pidx = pd.period_range(\"2003-01\", \"2025-07\", freq=\"M\")\n",
    "\n",
    "    # Resume if an output file already exists\n",
    "    if os.path.exists(out_path):\n",
    "        prev = pd.read_csv(out_path, index_col=0)\n",
    "        # Convert index back to PeriodIndex if it isn't already\n",
    "        try:\n",
    "            prev.index = pd.PeriodIndex(prev.index, freq=\"M\")\n",
    "        except Exception:\n",
    "            # If index looks like timestamps, convert then to Period\n",
    "            prev.index = pd.to_datetime(prev.index).to_period(\"M\")\n",
    "        # Align to our master grid\n",
    "        prev = prev.reindex(full_pidx)\n",
    "        done = set(prev.columns)\n",
    "        df = prev\n",
    "        _append_log({\"event\": \"resume\", \"columns\": sorted(list(done)), \"n_cols\": len(done)})\n",
    "        print(f\"Resuming from '{out_path}' with {len(done)} columns.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(index=full_pidx)\n",
    "        done = set()\n",
    "        print(\"Starting fresh.\")\n",
    "\n",
    "    skipped = []\n",
    "    attempted = 0\n",
    "    added = 0\n",
    "\n",
    "    for t in tickers:\n",
    "        if t in done:\n",
    "            continue\n",
    "\n",
    "        attempted += 1\n",
    "        # simple backoff loop for transient rate-limit errors\n",
    "        for attempt in range(4):  # up to 4 tries with backoff\n",
    "            try:\n",
    "                px = _download_monthly_adjclose(t, start=start, end=end)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                wait = 2 ** attempt\n",
    "                _append_log({\"event\": \"exception\", \"ticker\": t, \"attempt\": attempt+1, \"error\": repr(e)})\n",
    "                time.sleep(wait)\n",
    "        else:\n",
    "            px = None  # all attempts failed\n",
    "\n",
    "        if px is None or len(px) <= 2:\n",
    "            skipped.append(t)\n",
    "            _append_log({\"event\": \"skip\", \"ticker\": t, \"reason\": \"no_data_or_short\"})\n",
    "            continue\n",
    "\n",
    "        # compute monthly returns; ensure Series\n",
    "        rets = px.pct_change().dropna()\n",
    "        if isinstance(rets, pd.DataFrame):\n",
    "            rets = rets.squeeze()\n",
    "\n",
    "        # column DF named by original ticker, aligned to master grid\n",
    "        col_df = pd.DataFrame(rets)\n",
    "        col_df.columns = [t]\n",
    "        col_df = col_df.reindex(full_pidx)\n",
    "\n",
    "        # Merge into our accumulating DF (preserving any existing columns)\n",
    "        df[t] = col_df[t]\n",
    "        added += 1\n",
    "        done.add(t)\n",
    "\n",
    "        # Optional prune ultra-sparse columns on the fly\n",
    "        if min_non_na > 1 and df[t].count() < min_non_na:\n",
    "            # remove and mark skipped if too sparse\n",
    "            df.drop(columns=[t], inplace=True)\n",
    "            done.discard(t)\n",
    "            skipped.append(t)\n",
    "            _append_log({\"event\": \"drop_sparse\", \"ticker\": t, \"non_na\": int(col_df[t].count())})\n",
    "        else:\n",
    "            # Checkpoint after each successful add\n",
    "            _atomic_save_csv(df, out_path)\n",
    "            _append_log({\"event\": \"checkpoint\", \"ticker\": t, \"n_cols\": int(df.shape[1])})\n",
    "            print(f\"Saved checkpoint with '{t}' (now {df.shape[1]} cols).\")\n",
    "\n",
    "    if not df.columns.tolist():\n",
    "        print(\"No tickers returned data.\")\n",
    "        # keep index as requested\n",
    "        if not as_period_index:\n",
    "            out = df.copy()\n",
    "            out.index = out.index.to_timestamp('M')\n",
    "            return out\n",
    "        return df\n",
    "\n",
    "    # Final tidy: drop ultra-sparse columns globally if needed\n",
    "    if min_non_na > 1:\n",
    "        keep = df.columns[df.count() >= min_non_na]\n",
    "        if len(keep) != df.shape[1]:\n",
    "            dropped = sorted(set(df.columns) - set(keep))\n",
    "            df = df[keep]\n",
    "            _append_log({\"event\": \"final_drop_sparse\", \"dropped\": dropped})\n",
    "\n",
    "    if skipped:\n",
    "        print(f\"Skipped (no/short data): {len(skipped)} â†’ {sorted(skipped)[:20]}{' ...' if len(skipped)>20 else ''}\")\n",
    "    print(f\"Built monthly returns for {df.shape[1]} tickers, {df.shape[0]} months. \"\n",
    "          f\"(attempted: {attempted}, added: {added})\")\n",
    "\n",
    "    # Save final snapshot (idempotent)\n",
    "    _atomic_save_csv(df, out_path)\n",
    "    _append_log({\"event\": \"final_save\", \"n_cols\": int(df.shape[1])})\n",
    "\n",
    "    # Return with desired index type\n",
    "    if as_period_index:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out.index = out.index.to_timestamp('M')  # month-end timestamps\n",
    "    return out\n",
    "\n",
    "# -------- Example usage --------\n",
    "# all_tickers = [\"AAPL\", \"MSFT\", \"BRK.B\", \"GOOG\", \"ACEL.\", \"BFTC\", \"XOM\", \"BRK.A\"]\n",
    "df_mret = monthly_returns_df(all_tickers, out_path=OUT_PATH)\n",
    "print(df_mret.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c077f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
